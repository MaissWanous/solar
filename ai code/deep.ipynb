{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1e64ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laptop Land\\Desktop\\projects\\solar-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load pretrained sentiment analysis pipeline\n",
    "model_name = \"cardiffnlp/twitter-roberta-base-sentiment\"\n",
    "classifier = pipeline(\"sentiment-analysis\", model=model_name, tokenizer=model_name)\n",
    "\n",
    "# English texts to classify\n",
    "texts = [\n",
    "    \"I love this product! It works perfectly.\",\n",
    "    \"This is the worst experience I've ever had.\",\n",
    "    \"It's okay, nothing special but not terrible.\",\n",
    "    \"Absolutely fantastic service and support!\",\n",
    "    \"I'm really disappointed with the quality.\"\n",
    "]\n",
    "\n",
    "# Run classification\n",
    "for text in texts:\n",
    "    result = classifier(text)\n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Sentiment: {result[0]['label']} (Score: {result[0]['score']:.4f})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0340a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Laptop Land\\Desktop\\projects\\solar-main\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "# from transformers import (\n",
    "#     RobertaTokenizerFast,\n",
    "#     RobertaForSequenceClassification,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorWithPadding,\n",
    "#     EarlyStoppingCallback\n",
    "# )\n",
    "\n",
    "# # 1. Data Loading & Initial Cleanup\n",
    "# def load_and_clean_csv(path: str) -> pd.DataFrame:\n",
    "#     df = pd.read_csv(path, encoding='latin-1')\n",
    "#     df = df.drop(columns=[\"product_name\", \"product_price\", \"Rate\"])\n",
    "#     df[\"Review\"]    = df[\"Review\"].astype(\"string\")\n",
    "#     df[\"Summary\"]   = df[\"Summary\"].astype(\"string\")\n",
    "#     df[\"Sentiment\"] = df[\"Sentiment\"].astype(\"category\")\n",
    "#     return df\n",
    "\n",
    "# # 2. Text Cleaning Utility\n",
    "# def clean_text_column(column: pd.Series) -> pd.Series:\n",
    "#     return (\n",
    "#         column\n",
    "#         .str.strip()\n",
    "#         .str.replace(\"\\xa0\", \"\", regex=False)\n",
    "#         .replace(r\"^\\s*(nan|NaN|NAN)\\s*$\", np.nan, regex=True)\n",
    "#     )\n",
    "\n",
    "# # 3. Full Preprocessing Pipeline\n",
    "# def preprocess(df: pd.DataFrame):\n",
    "#     df[\"Review\"]  = clean_text_column(df[\"Review\"])\n",
    "#     df[\"Summary\"] = clean_text_column(df[\"Summary\"])\n",
    "#     df = df.dropna(subset=[\"Review\", \"Summary\", \"Sentiment\"]).reset_index(drop=True)\n",
    "#     df[\"text\"] = df[\"Summary\"] + \" \" + df[\"Review\"]\n",
    "#     le = LabelEncoder()\n",
    "#     df[\"label\"] = le.fit_transform(df[\"Sentiment\"])\n",
    "#     return df, le\n",
    "\n",
    "# # 4. Train / Validation / Test Split\n",
    "# def split_data(df: pd.DataFrame):\n",
    "#     train_df, temp_df = train_test_split(\n",
    "#         df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    "#     )\n",
    "#     val_df, test_df = train_test_split(\n",
    "#         temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42\n",
    "#     )\n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "# # 5. Dataset for Hugging Face\n",
    "# class ReviewDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "#         self.texts      = texts.tolist()\n",
    "#         self.labels     = labels.tolist()\n",
    "#         self.tokenizer  = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         enc = self.tokenizer(\n",
    "#             self.texts[idx],\n",
    "#             truncation=True,\n",
    "#             max_length=self.max_length\n",
    "#         )\n",
    "#         enc[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return enc\n",
    "\n",
    "# # 6. Metrics Computation\n",
    "# def compute_metrics(pred):\n",
    "#     preds = pred.predictions.argmax(axis=-1)\n",
    "#     labels = pred.label_ids\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy_score(labels, preds),\n",
    "#         \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
    "#     }\n",
    "\n",
    "# # 7. Training & Evaluation\n",
    "# def train_and_evaluate(train_ds, val_ds, test_ds, tokenizer, num_labels=3):\n",
    "#     model = RobertaForSequenceClassification.from_pretrained(\n",
    "#         \"roberta-base\", num_labels=num_labels\n",
    "#     )\n",
    "\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"roberta_sentiment_fast\",\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"f1_macro\",\n",
    "#         num_train_epochs=5,  # ممكن يوقف قبل ما يخلص الخمسة\n",
    "#         per_device_train_batch_size=32,\n",
    "#         per_device_eval_batch_size=64,\n",
    "#         logging_steps=50,\n",
    "#         fp16=True\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_ds,\n",
    "#         eval_dataset=val_ds,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]  \n",
    "#         # بيوقف إذا ما تحسن الأداء بعد عصرين متتاليين\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "#     preds_output = trainer.predict(test_ds)\n",
    "#     preds = preds_output.predictions.argmax(axis=-1)\n",
    "#     labels = preds_output.label_ids\n",
    "\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     f1  = f1_score(labels, preds, average=\"macro\")\n",
    "#     cm  = confusion_matrix(labels, preds)\n",
    "\n",
    "#     print(f\"Test Accuracy: {acc:.4f}\")\n",
    "#     print(f\"Test F1-Macro: {f1:.4f}\")\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(cm)\n",
    "\n",
    "#     return trainer\n",
    "\n",
    "# # 8. Orchestrator\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_raw       = load_and_clean_csv(\"Equal.csv\")\n",
    "#     df_clean, le = preprocess(df_raw)\n",
    "#     train_df, val_df, test_df = split_data(df_clean)\n",
    "\n",
    "#     tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "#     train_ds  = ReviewDataset(train_df[\"text\"], train_df[\"label\"], tokenizer)\n",
    "#     val_ds    = ReviewDataset(val_df[\"text\"],   val_df[\"label\"],   tokenizer)\n",
    "#     test_ds   = ReviewDataset(test_df[\"text\"],  test_df[\"label\"],  tokenizer)\n",
    "\n",
    "#     trainer = train_and_evaluate(train_ds, val_ds, test_ds, tokenizer, num_labels=len(le.classes_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8501ed20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# from torch.utils.data import Dataset\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import LabelEncoder\n",
    "# from sklearn.metrics import accuracy_score, f1_score, confusion_matrix\n",
    "# from transformers import (\n",
    "#     RobertaTokenizerFast,\n",
    "#     RobertaForSequenceClassification,\n",
    "#     Trainer,\n",
    "#     TrainingArguments,\n",
    "#     DataCollatorWithPadding,\n",
    "#     EarlyStoppingCallback\n",
    "# )\n",
    "\n",
    "# # 1. Load & Clean CSV\n",
    "# def load_and_clean_csv(path: str) -> pd.DataFrame:\n",
    "#     df = pd.read_csv(path, encoding='latin-1')\n",
    "#     df = df.drop(columns=[\"product_name\", \"product_price\", \"Rate\"])\n",
    "#     df[\"Review\"]    = df[\"Review\"].astype(\"string\")\n",
    "#     df[\"Summary\"]   = df[\"Summary\"].astype(\"string\")\n",
    "#     df[\"Sentiment\"] = df[\"Sentiment\"].astype(\"category\")\n",
    "#     return df\n",
    "\n",
    "# # 2. Clean Text Column\n",
    "# def clean_text_column(column: pd.Series) -> pd.Series:\n",
    "#     return (\n",
    "#         column\n",
    "#         .str.strip()\n",
    "#         .str.replace(\"\\xa0\", \"\", regex=False)\n",
    "#         .replace(r\"^\\s*(nan|NaN|NAN)\\s*$\", np.nan, regex=True)\n",
    "#     )\n",
    "\n",
    "# # 3. Preprocess Data\n",
    "# def preprocess(df: pd.DataFrame):\n",
    "#     df[\"Review\"]  = clean_text_column(df[\"Review\"])\n",
    "#     df[\"Summary\"] = clean_text_column(df[\"Summary\"])\n",
    "#     df = df.dropna(subset=[\"Review\", \"Summary\", \"Sentiment\"]).reset_index(drop=True)\n",
    "#     df[\"text\"] = df[\"Summary\"] + \" \" + df[\"Review\"]\n",
    "\n",
    "#     le = LabelEncoder()\n",
    "#     df[\"label\"] = le.fit_transform(df[\"Sentiment\"])\n",
    "#     return df, le\n",
    "\n",
    "# # 4. Split Data\n",
    "# def split_data(df: pd.DataFrame):\n",
    "#     train_df, temp_df = train_test_split(\n",
    "#         df, test_size=0.2, stratify=df[\"label\"], random_state=42\n",
    "#     )\n",
    "#     val_df, test_df = train_test_split(\n",
    "#         temp_df, test_size=0.5, stratify=temp_df[\"label\"], random_state=42\n",
    "#     )\n",
    "#     return train_df, val_df, test_df\n",
    "\n",
    "# # 5. Custom Dataset\n",
    "# class ReviewDataset(Dataset):\n",
    "#     def __init__(self, texts, labels, tokenizer, max_length=128):\n",
    "#         self.texts      = texts.tolist()\n",
    "#         self.labels     = labels.tolist()\n",
    "#         self.tokenizer  = tokenizer\n",
    "#         self.max_length = max_length\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.labels)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         enc = self.tokenizer(\n",
    "#             self.texts[idx],\n",
    "#             truncation=True,\n",
    "#             max_length=self.max_length\n",
    "#         )\n",
    "#         enc[\"labels\"] = torch.tensor(self.labels[idx], dtype=torch.long)\n",
    "#         return enc\n",
    "\n",
    "# # 6. Metrics\n",
    "# def compute_metrics(pred):\n",
    "#     preds = pred.predictions.argmax(axis=-1)\n",
    "#     labels = pred.label_ids\n",
    "#     return {\n",
    "#         \"accuracy\": accuracy_score(labels, preds),\n",
    "#         \"f1_macro\": f1_score(labels, preds, average=\"macro\")\n",
    "#     }\n",
    "\n",
    "# # 7. Train & Evaluate with Auto-Save\n",
    "# def train_and_evaluate(train_ds, val_ds, test_ds, tokenizer, num_labels=3):\n",
    "#     model = RobertaForSequenceClassification.from_pretrained(\n",
    "#         \"roberta-base\", num_labels=num_labels\n",
    "#     )\n",
    "\n",
    "#     data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"roberta_sentiment_best\",\n",
    "#         evaluation_strategy=\"epoch\",\n",
    "#         save_strategy=\"epoch\",\n",
    "#         save_total_limit=1,  # الاحتفاظ بأفضل نسخة فقط\n",
    "#         load_best_model_at_end=True,\n",
    "#         metric_for_best_model=\"f1_macro\",\n",
    "#         num_train_epochs=5,\n",
    "#         per_device_train_batch_size=32,\n",
    "#         per_device_eval_batch_size=64,\n",
    "#         logging_steps=50,\n",
    "#         fp16=True\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=train_ds,\n",
    "#         eval_dataset=val_ds,\n",
    "#         compute_metrics=compute_metrics,\n",
    "#         tokenizer=tokenizer,\n",
    "#         data_collator=data_collator,\n",
    "#         callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]\n",
    "#     )\n",
    "\n",
    "#     trainer.train()\n",
    "\n",
    "#     # حفظ أفضل نموذج و Tokenizer\n",
    "#     save_path = \"saved_roberta_model\"\n",
    "#     trainer.save_model(save_path)\n",
    "#     tokenizer.save_pretrained(save_path)\n",
    "#     print(f\"✅ تم حفظ النموذج و الـ Tokenizer في المجلد: {save_path}\")\n",
    "\n",
    "#     # تقييم على الاختبار\n",
    "#     preds_output = trainer.predict(test_ds)\n",
    "#     preds = preds_output.predictions.argmax(axis=-1)\n",
    "#     labels = preds_output.label_ids\n",
    "\n",
    "#     acc = accuracy_score(labels, preds)\n",
    "#     f1  = f1_score(labels, preds, average=\"macro\")\n",
    "#     cm  = confusion_matrix(labels, preds)\n",
    "\n",
    "#     print(f\"Test Accuracy: {acc:.4f}\")\n",
    "#     print(f\"Test F1-Macro: {f1:.4f}\")\n",
    "#     print(\"Confusion Matrix:\")\n",
    "#     print(cm)\n",
    "\n",
    "#     return trainer\n",
    "\n",
    "# # 8. Orchestrator\n",
    "# if __name__ == \"__main__\":\n",
    "#     df_raw       = load_and_clean_csv(\"Equal.csv\")\n",
    "#     df_clean, le = preprocess(df_raw)\n",
    "#     train_df, val_df, test_df = split_data(df_clean)\n",
    "\n",
    "#     tokenizer = RobertaTokenizerFast.from_pretrained(\"roberta-base\")\n",
    "\n",
    "#     train_ds  = ReviewDataset(train_df[\"text\"], train_df[\"label\"], tokenizer)\n",
    "#     val_ds    = ReviewDataset(val_df[\"text\"],   val_df[\"label\"],   tokenizer)\n",
    "#     test_ds   = ReviewDataset(test_df[\"text\"],  test_df[\"label\"],  tokenizer)\n",
    "\n",
    "#     trainer = train_and_evaluate(train_ds, val_ds, test_ds, tokenizer, num_labels=len(le.classes_))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.2)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
